# model_build_and_compare.py
# Copyright (c) 2026, Alibaba Cloud. All rights reserved.
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GroupShuffleSplit
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import product
from matplotlib.lines import Line2D
from joblib import Parallel, delayed
from tqdm import tqdm
import os

# ==============================================================================
# 1. Environment and Plotting Configuration
# ==============================================================================
# Configures the visual style for all plots generated by the script.
sns.set_style("whitegrid")
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ==============================================================================
# 2. Data Cleaning and Feature Engineering
# ==============================================================================
class WorkloadOutlierCleaner:
    """
    A class to identify and remove outliers from workload data.
    
    Outliers are detected for each unique workload configuration based on the
    Interquartile Range (IQR) of the 'duration_us' metric. A workload is
    defined by the combination of columns in `identity_cols`.
    """
    def __init__(self, multiplier=1.5):
        """
        Initializes the cleaner.

        Args:
            multiplier (float, optional): The IQR multiplier to define outlier bounds. 
                                          A value of 1.5 is standard. Set to np.inf to disable outlier removal.
        """
        self.multiplier = np.inf if multiplier is None or np.isinf(multiplier) else multiplier
        self.bounds_ = {}
        # Defines the set of columns that constitute a unique workload profile.
        self.identity_cols = ['compute_batch_size', 'compute_avg_input_length', 'compute_max_input_length', 'compute_forward_mode', 'post_batch_size', 'post_forward_mode']
    
    def fit(self, df):
        """
        Calculates the outlier detection bounds from the training data.
        
        For each unique workload group, it computes the lower and upper bounds
        for 'duration_us' based on the specified IQR multiplier.
        
        Args:
            df (pd.DataFrame): The dataframe to learn the bounds from.
        
        Returns:
            WorkloadOutlierCleaner: The fitted instance of the cleaner.
        """
        if self.multiplier == np.inf or df.empty: return self
        df_copy = df.copy(); df_copy[self.identity_cols] = df_copy[self.identity_cols].fillna(0)
        for name, group in df_copy.groupby(self.identity_cols):
            if len(group) < 4: continue
            q1, q3 = np.percentile(group['duration_us'], [25, 75]); iqr = q3 - q1
            self.bounds_[name] = (q1 - self.multiplier * iqr, q3 + self.multiplier * iqr)
        return self

    def transform(self, df):
        """
        Removes outliers from the dataframe based on the pre-computed bounds.
        
        Args:
            df (pd.DataFrame): The dataframe to clean.
            
        Returns:
            pd.DataFrame: A new dataframe with outliers removed.
        """
        if self.multiplier == np.inf or not self.bounds_ or df.empty: return df
        df_copy = df.copy(); df_copy[self.identity_cols] = df_copy[self.identity_cols].fillna(0)
        retained_indices = []
        for name, group in df_copy.groupby(self.identity_cols):
            if name in self.bounds_:
                lower, upper = self.bounds_[name]
                retained_indices.extend(group[(group['duration_us'] >= lower) & (group['duration_us'] <= upper)].index)
            else: retained_indices.extend(group.index)
        return df.loc[retained_indices].copy()

    def fit_transform(self, df):
        """
        A convenience method that fits the cleaner and transforms the data in one step.
        
        Args:
            df (pd.DataFrame): The dataframe to fit and transform.
        
        Returns:
            pd.DataFrame: A new dataframe with outliers removed.
        """
        self.fit(df); return self.transform(df)

def get_features(df, config, mode=None):
    """
    Engineers physics-informed features from the raw workload data.

    This function creates features that reflect the underlying computational and
    memory loads of a transformer model, aiming to provide the model with more
    meaningful signals than raw inputs alone. It distinguishes between the
    computational patterns of the 'prefill' and 'decode' stages.

    Args:
        df (pd.DataFrame): The input dataframe containing raw workload metrics.
        config (dict): The experiment configuration dictionary.
        mode (str, optional): The specific mode ('prefill' or 'decode') if building
                              features for a split model.

    Returns:
        pd.DataFrame: A dataframe containing the engineered features.
    """
    # Select the relevant raw columns for feature engineering.
    cols = [
        'compute_batch_size', 
        'compute_avg_input_length', 
        'compute_avg_output_length',
        'compute_forward_mode'
    ]
    # Include post_batch_size as it's a verified key factor.
    cols.append('post_batch_size')
    
    X = df[cols].copy().fillna(0)
    epsilon = 1e-9

    # --- Physics-Informed Feature Construction ---

    # The effective total sequence length, combining input and output tokens.
    X['real_kv_len'] = X['compute_avg_input_length'] + X['compute_avg_output_length']
    
    # [Decode Phase] KV Cache Load: Represents memory bandwidth pressure.
    # This is proportional to Prompt (Input) + Generated (Output)
    X['workload_kv_cache'] = (X['compute_forward_mode'] == 2).astype(int) * X['compute_batch_size'] * X['real_kv_len']
    
    # [Prefill Phase] Attention Complexity: Represents the quadratic computational cost of self-attention.
    # This is proportional to sequence_length^2.
    X['workload_attn_quad'] = (X['compute_forward_mode'] == 1).astype(int) * X['compute_batch_size'] * (X['real_kv_len'] ** 2)
    
    # [Prefill Phase] FFN/MatMul Compute Load: Represents the work done in linear layers.
    # This is proportional to the total number of input tokens being processed.
    X['workload_compute_tokens'] = (X['compute_forward_mode'] == 1).astype(int) * X['compute_batch_size'] * X['compute_avg_input_length']

    return X


# ==============================================================================
# 3. Experiment Execution and Evaluation
# ==============================================================================
def get_metrics(y_true, y_pred, k):
    """
    Calculates a comprehensive set of regression performance metrics.

    Args:
        y_true (np.array): The ground truth values.
        y_pred (np.array): The predicted values from the model.
        k (int): The number of independent variables in the model, used for
                 calculating Adjusted R². Can be np.nan if not applicable.

    Returns:
        dict: A dictionary containing metric names and their calculated values.
              Returns a dictionary of NaNs if calculation is not possible.
    """
    y_pred = np.nan_to_num(y_pred.clip(0)); n = len(y_true)
    empty_metrics = {'R²': np.nan, 'Adj R²': np.nan, 'RMSE': np.nan, 'Resid Mean': np.nan, 'Resid Std': np.nan, 'MPE': np.nan, 'MAPE': np.nan, 'MdAPE': np.nan, 'MAPE Std': np.nan, 'P95 APE': np.nan, '% In 10%': np.nan, '% In 25%': np.nan}
    if n < 2 or np.all(y_true < 1e-9): return empty_metrics
    residuals = y_true - y_pred; epsilon = 1e-9; percentage_errors = residuals / (y_true + epsilon); abs_percentage_errors = np.abs(percentage_errors); r2 = r2_score(y_true, y_pred); adj_r2 = np.nan
    if not np.isnan(k) and n > k + 1: adj_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
    return {'R²': r2, 'Adj R²': adj_r2, 'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)), 'Resid Mean': np.mean(residuals), 'Resid Std': np.std(residuals), 'MPE': np.mean(percentage_errors), 'MAPE': np.mean(abs_percentage_errors), 'MdAPE': np.median(abs_percentage_errors), 'MAPE Std': np.std(abs_percentage_errors), 'P95 APE': np.percentile(abs_percentage_errors, 95), '% In 10%': np.mean(abs_percentage_errors <= 0.10), '% In 25%': np.mean(abs_percentage_errors <= 0.25)}

def execute_and_evaluate_strategy(df_normal, df_abnormal, config):
    """
    Executes a full modeling and evaluation pipeline for a single experiment configuration.

    This function orchestrates the entire process for one strategy, defined by the `config`
    dictionary. The process includes:
    1. Splitting data into training and testing sets based on the specified strategy.
    2. Applying outlier cleaning either before or after the split.
    3. Training a model (either a single 'unified' model or separate models for 'prefill'
       and 'decode' phases).
    4. Evaluating the model's performance on the test set.
    5. Evaluating the trained model's generalization ability on a separate 'abnormal' dataset.
    6. Storing the results, predictions, and trained models.

    Args:
        df_normal (pd.DataFrame): The primary dataset for training and testing.
        df_abnormal (pd.DataFrame): A dataset with out-of-distribution or unusual
                                    workloads to test model robustness.
        config (dict): A dictionary specifying all parameters for the experiment,
                       such as model type, feature engineering approach, splitting
                       strategy, etc.

    Returns:
        dict: A dictionary containing the configuration, evaluation metrics for both
              test and abnormal data, predictions, and the trained model(s).
    """
    results = {'config': config, 'test_results': {}, 'abnormal_results': {}, 'trained_models': {}}
    def get_model(config):
        if config['model_type'] == 'GBDT':
            return GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=4, random_state=42)
        elif config['model_type'] == 'PolynomialRidge':
            if config['feature_type'] == 'domain':
                return make_pipeline(StandardScaler(), Ridge(alpha=1.0, max_iter=10000))
            else:
                return make_pipeline(PolynomialFeatures(degree=2, include_bias=False), StandardScaler(), Ridge(alpha=1.0, max_iter=10000))
    def process_and_train(train_df, test_df, config, mode=None):
        X_train = get_features(train_df, config, mode); X_test = get_features(test_df, config, mode)
        train_cols = X_train.columns; test_cols = X_test.columns
        missing_in_test = set(train_cols) - set(test_cols)
        for c in missing_in_test: X_test[c] = 0
        missing_in_train = set(test_cols) - set(train_cols)
        for c in missing_in_train: X_train[c] = 0
        X_test = X_test[train_cols]
        k = X_train.shape[1]; y_train_raw = train_df['duration_us'].values; y_test_raw = test_df['duration_us'].values
        y_train_processed = np.log1p(y_train_raw) if config['log_transform'] else y_train_raw
        if len(X_train) < 20: return None, None, None, k
        model = get_model(config).fit(X_train, y_train_processed)
        if X_test.empty: return model, np.array([]), np.array([]), k
        y_pred = model.predict(X_test)
        y_pred_final = np.expm1(y_pred) if config['log_transform'] else y_pred
        return model, y_test_raw, y_pred_final, k
    train_df, test_df = pd.DataFrame(), pd.DataFrame()
    cleaner = WorkloadOutlierCleaner(multiplier=config['iqr_multiplier'])
    train_df_raw, test_df_raw = pd.DataFrame(), pd.DataFrame()
    if config['split_strategy'] == 'random':
        if not df_normal.empty: train_df_raw, test_df_raw = train_test_split(df_normal, test_size=0.2, random_state=42)
    elif config['split_strategy'] == 'group':
        if not df_normal.empty:
            groups = pd.factorize(df_normal[cleaner.identity_cols].fillna(0).apply(tuple, axis=1))[0]
            splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
            train_idx, test_idx = next(splitter.split(df_normal, groups=groups))
            train_df_raw, test_df_raw = df_normal.iloc[train_idx], df_normal.iloc[test_idx]
    if config['cleaning_order'] == 'clean_first':
        cleaned_full_df = cleaner.fit_transform(df_normal)
        if not cleaned_full_df.empty:
            if config['split_strategy'] == 'random': train_df, test_df = train_test_split(cleaned_full_df, test_size=0.2, random_state=42)
            else:
                groups = pd.factorize(cleaned_full_df[cleaner.identity_cols].fillna(0).apply(tuple, axis=1))[0]
                splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
                train_idx, test_idx = next(splitter.split(cleaned_full_df, groups=groups))
                train_df, test_df = cleaned_full_df.iloc[train_idx], cleaned_full_df.iloc[test_idx]
    elif config['cleaning_order'] == 'split_first':
        train_df = cleaner.fit_transform(train_df_raw); test_df = cleaner.transform(test_df_raw)
    if train_df.empty: return results
    models = {}
    if config['approach'] == 'unified':
        model, y_true, y_pred, k = process_and_train(train_df, test_df, config)
        if model: models['unified'] = model
        if y_true is not None and len(y_true) > 0:
            results['test_results']['overall_metrics'] = get_metrics(y_true, y_pred, k)
            results['test_results']['predictions'] = {'y_true': y_true, 'y_pred': y_pred, 'indices': test_df.index}
            prefill_mask = (test_df['compute_forward_mode'] == 1); decode_mask = (test_df['compute_forward_mode'] == 2)
            results['test_results']['prefill_metrics'] = get_metrics(y_true[prefill_mask], y_pred[prefill_mask], k)
            results['test_results']['decode_metrics'] = get_metrics(y_true[decode_mask], y_pred[decode_mask], k)
    else:
        all_y_true, all_y_pred, all_indices = [], [], []
        for mode, mode_id in [('prefill', 1), ('decode', 2)]:
            train_df_mode = train_df[train_df.compute_forward_mode == mode_id]; test_df_mode = test_df[test_df.compute_forward_mode == mode_id]
            if len(train_df_mode) < 20: continue
            model, y_true, y_pred, k = process_and_train(train_df_mode, test_df_mode, config, mode=mode)
            if model: models[mode] = model
            if y_true is not None and len(y_true) > 0:
                results['test_results'][f'{mode}_metrics'] = get_metrics(y_true, y_pred, k)
                all_y_true.append(y_true); all_y_pred.append(y_pred); all_indices.append(test_df_mode.index)
        if all_y_true:
            y_true_concat, y_pred_concat = np.concatenate(all_y_true), np.concatenate(all_y_pred)
            results['test_results']['overall_metrics'] = get_metrics(y_true_concat, y_pred_concat, k=np.nan)
            results['test_results']['predictions'] = {'y_true': y_true_concat, 'y_pred': y_pred_concat, 'indices': np.concatenate(all_indices)}
    if not df_abnormal.empty and models:
        y_true_ab, y_pred_ab, indices_ab = [], [], []
        if config['approach'] == 'unified' and 'unified' in models:
            model, X_ab = models['unified'], get_features(df_abnormal, config)
            if not X_ab.empty:
                train_cols = model.feature_names_in_
                missing_in_ab = set(train_cols) - set(X_ab.columns)
                for c in missing_in_ab: X_ab[c] = 0
                X_ab = X_ab[train_cols]
                pred = model.predict(X_ab)
                y_pred_ab.append(np.expm1(pred) if config['log_transform'] else pred)
                y_true_ab.append(df_abnormal['duration_us'].values); indices_ab.append(df_abnormal.index)
        else:
            for mode, mode_id in [('prefill', 1), ('decode', 2)]:
                if mode in models:
                    model, abnormal_mode_df = models[mode], df_abnormal[df_abnormal.compute_forward_mode == mode_id]
                    if not abnormal_mode_df.empty:
                        X_ab_mode = get_features(abnormal_mode_df, config, mode=mode)
                        train_cols = model.feature_names_in_
                        missing_in_ab = set(train_cols) - set(X_ab_mode.columns)
                        for c in missing_in_ab: X_ab_mode[c] = 0
                        X_ab_mode = X_ab_mode[train_cols]
                        pred = model.predict(X_ab_mode)
                        y_pred_ab.append(np.expm1(pred) if config['log_transform'] else pred)
                        y_true_ab.append(abnormal_mode_df['duration_us'].values); indices_ab.append(abnormal_mode_df.index)
        if y_true_ab:
            y_true_ab_concat, y_pred_ab_concat = np.concatenate(y_true_ab), np.concatenate(y_pred_ab)
            results['abnormal_results']['overall_metrics'] = get_metrics(y_true_ab_concat, y_pred_ab_concat, k=np.nan)
            results['abnormal_results']['predictions'] = {'y_true': y_true_ab_concat, 'y_pred': y_pred_ab_concat, 'indices': np.concatenate(indices_ab)}
    
    results['trained_models'] = models
    return results

# ==============================================================================
# 4. Results Display and Visualization
# ==============================================================================

def generate_summary_table(results):
    """
    Generates a comprehensive summary table from all experiment results.

    This function aggregates the results from multiple experiment runs, formats them
    into a pandas DataFrame for easy comparison, prints a condensed version of the
    key metrics to the console, and saves the full table to a CSV file.

    Args:
        results (list): A list of result dictionaries, one for each experiment run.

    Returns:
        pd.DataFrame: A DataFrame containing the formatted and sorted summary of
                      all experiments. Returns None if no results are provided.
    """
    table_data = []
    metric_names = ['R²', 'Adj R²', 'RMSE', 'Resid Mean', 'Resid Std', 'MPE', 'MAPE', 'MdAPE', 'MAPE Std', 'P95 APE', '% In 10%', '% In 25%']
    data_sources = {'T': 'test_results', 'A': 'abnormal_results'}; segments = {'O': 'overall_metrics', 'P': 'prefill_metrics', 'D': 'decode_metrics'}
    for res in filter(None, results):
        config = res.get('config', {});
        if not config: continue
        iqr_multiplier = config.get('iqr_multiplier', np.inf); cleaning_order_text = '-' if iqr_multiplier == np.inf else ('Clean First' if config.get('cleaning_order') == 'clean_first' else 'Split First'); kv_mode_text = '-'
        if config['feature_type'] == 'domain': kv_mode_text = config.get('decode_kv_load_mode', 'N/A')
        row = {'Model': 'Poly' if config['model_type'] == 'PolynomialRidge' else 'GBDT', 'Features': config['feature_type'],'Approach': 'Split' if config['approach'] == 'split' else 'Unified', 'KV Mode': kv_mode_text,'Log': '✅' if config['log_transform'] else '❌','IQR': 'None' if iqr_multiplier == np.inf else iqr_multiplier, 'Cleaning Order': cleaning_order_text,'Split Strategy': 'Group' if config['split_strategy'] == 'group' else 'Random',}
        for src_short, src_long in data_sources.items():
            for seg_short, seg_long in segments.items():
                if src_short == 'A' and seg_short != 'O': continue
                for metric in metric_names:
                    col_name = f"{metric}({src_short},{seg_short})"; row[col_name] = res.get(src_long, {}).get(seg_long, {}).get(metric, np.nan)
        row['name'] = config['name']; table_data.append(row)
    if not table_data: print("No results available for analysis."); return None
    summary_df = pd.DataFrame(table_data).set_index('name').sort_values(by=['R²(T,O)'], ascending=[False])
    base_cols = ['Model', 'Features', 'Approach', 'KV Mode', 'Split Strategy', 'Log', 'IQR', 'Cleaning Order']
    print("\n" + "="*170 + "\n### Experiment Results Summary (Key Metrics Preview) ###\n" + "="*170)
    formatters = {}
    for col in summary_df.columns:
        if any(p in col for p in ['MPE', 'MAPE', 'MdAPE', 'P95 APE', '% In']): formatters[col] = '{:.2%}'.format
        elif any(p in col for p in ['R²', 'Adj R²']): formatters[col] = '{:.4f}'.format
        elif any(p in col for p in ['RMSE', 'Resid']): formatters[col] = '{:.2f}'.format
    display_cols = base_cols + ['R²(T,O)', 'MAPE(T,O)', 'R²(A,O)', 'MAPE(A,O)', 'R²(T,P)', 'MAPE(T,P)', 'R²(T,D)', 'MAPE(T,D)']
    print(summary_df[[c for c in display_cols if c in summary_df.columns]].to_string(formatters=formatters, na_rep='-'))
    try:
        csv_filename = 'experiment_summary.csv'; summary_df.to_csv(csv_filename, encoding='utf-8-sig', float_format='%.6f')
        print(f"\n✅ Full experiment results successfully exported to: {csv_filename}")
    except Exception as e: print(f"\n❌ Failed to export CSV file: {e}")
    return summary_df

def analyze_champion_models(all_results, summary_df):
    """
    Analyzes and explains the best-performing models ("champions").

    It identifies the top-performing GBDT and Polynomial/Linear models from the
    summary table based on a prioritized selection criteria. For each champion,
    it prints an interpretability report:
    - For GBDT models: Feature importances.
    - For Polynomial/Linear models: Model coefficients and the formula.

    Args:
        all_results (list): The list containing full results for all experiments.
        summary_df (pd.DataFrame): The summary table of experiment metrics.
    """
    if summary_df is None or summary_df.empty: return

    print("\n" + "="*80 + "\n### Champion Model Interpretability Analysis ###\n" + "="*80)

    priority_summary = summary_df[(summary_df['Split Strategy'] == 'Group') & (summary_df['Cleaning Order'] == 'Split First')]
    if priority_summary.empty: priority_summary = summary_df[summary_df['Cleaning Order'] == 'Split First']
    if priority_summary.empty: priority_summary = summary_df

    best_gbdt_res, best_poly_res = None, None
    for model_type, df_champs in [('GBDT', priority_summary[priority_summary['Model'] == 'GBDT']), ('Poly', priority_summary[priority_summary['Model'] == 'Poly'])]:
        df_champs_valid = df_champs.dropna(subset=['R²(T,O)'])
        if not df_champs_valid.empty:
            best_name = df_champs_valid.index[0]
            res = next((r for r in all_results if r and r['config']['name'] == best_name), None)
            if model_type == 'GBDT': best_gbdt_res = res
            else: best_poly_res = res
    
    def _analyze_poly(model, config, prefix=""):
        target_name = f"log(1+duration)" if config['log_transform'] else "duration"
        print(f"\n--- {prefix} Polynomial/Linear Model Formula Analysis ---")
        
        if 'polynomialfeatures' in model.named_steps:
            poly_features = model.named_steps['polynomialfeatures']
            ridge = model.named_steps['ridge']
            feature_names = poly_features.get_feature_names_out()
        else:
            ridge = model.named_steps['ridge']
            feature_names = model.named_steps['standardscaler'].get_feature_names_out()

        intercept = ridge.intercept_
        coeffs = ridge.coef_

        coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coeffs})
        coef_df['Abs_Coefficient'] = coef_df['Coefficient'].abs()
        coef_df = coef_df.sort_values(by='Abs_Coefficient', ascending=False)
        
        print(f"Formula: {target_name} = {intercept:.4f} + Σ (Coefficient * Standardized_Feature_Value)")
        print("Note: Coefficients apply to features after they have been standardized by StandardScaler.")
        print("\nTop 20 most important feature terms and their coefficients:")
        print(coef_df.head(20)[['Feature', 'Coefficient']].to_string(index=False, float_format="%.4f"))

    def _analyze_gbdt(model, config, prefix=""):
        print(f"\n--- {prefix} GBDT Model Feature Importance Analysis ---")
        importances = model.feature_importances_
        feature_names = model.feature_names_in_
        
        imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
        imp_df = imp_df.sort_values(by='Importance', ascending=False)

        print("\nTop 20 most important features and their importance scores:")
        print(imp_df.head(20).to_string(index=False, float_format="%.4f"))

    for res, model_name in [(best_poly_res, "Polynomial Regression"), (best_gbdt_res, "GBDT")]:
        if not res: 
            print(f"\nCould not find a valid champion {model_name} model to analyze.")
            continue
        
        print(f"\n\n--- Analyzing Champion {model_name} Model ---")
        print(f"Configuration: {res['config']['name']}")
        
        trained_models = res.get('trained_models', {})
        if not trained_models:
            print("Error: No trained models were saved in this experiment result.")
            continue

        analysis_func = _analyze_poly if "Polynomial" in model_name else _analyze_gbdt

        if 'unified' in trained_models:
            analysis_func(trained_models['unified'], res['config'], prefix="Unified Model")
        else:
            if 'prefill' in trained_models:
                analysis_func(trained_models['prefill'], res['config'], prefix="Prefill Sub-Model")
            if 'decode' in trained_models:
                analysis_func(trained_models['decode'], res['config'], prefix="Decode Sub-Model")

def plot_champion_models(results, summary_df, df_normal, df_abnormal):
    """
    Visualizes the performance of the champion models.

    This function generates a side-by-side scatter plot comparing the predicted vs.
    actual performance for the best GBDT and Polynomial/Linear models. The plot
    distinguishes between normal (prefill/decode) and abnormal data points to
    provide a clear view of model accuracy and generalization.

    Args:
        results (list): The list containing full results for all experiments.
        summary_df (pd.DataFrame): The summary table of experiment metrics.
        df_normal (pd.DataFrame): The original dataframe of normal data.
        df_abnormal (pd.DataFrame): The original dataframe of abnormal data.
    """
    if summary_df is None or summary_df.empty: return
    priority_summary = summary_df[(summary_df['Split Strategy'] == 'Group') & (summary_df['Cleaning Order'] == 'Split First')]
    if priority_summary.empty: priority_summary = summary_df[summary_df['Cleaning Order'] == 'Split First']
    if priority_summary.empty: priority_summary = summary_df
    best_gbdt_res, best_poly_res = None, None
    for model_type, df_champs in [('GBDT', priority_summary[priority_summary['Model'] == 'GBDT']), ('Poly', priority_summary[priority_summary['Model'] == 'Poly'])]:
        df_champs_valid = df_champs.dropna(subset=['R²(T,O)'])
        if not df_champs_valid.empty:
            best_name = df_champs_valid.index[0]
            res = next((r for r in results if r and r['config']['name'] == best_name), None)
            if model_type == 'GBDT': best_gbdt_res = res
            else: best_poly_res = res
    def get_title(res, model_name):
        config = res['config']; metrics_test = res['test_results']['overall_metrics']; metrics_abnormal = res.get('abnormal_results', {}).get('overall_metrics', {}); split_text = 'Group' if config['split_strategy'] == 'group' else 'Random'; kv_text = ""
        if config['feature_type'] == 'domain': kv_text = f", KV:{config.get('decode_kv_load_mode', 'N/A')}"
        return (f"Champion {model_name} (R² Normal: {metrics_test.get('R²', np.nan):.4f}, R² Abnormal: {metrics_abnormal.get('R²', np.nan):.4f})\n" f"{config['feature_type']} Features, {config['approach']} Approach{kv_text}, Log:{'✅' if config['log_transform'] else '❌'}, " f"Split:{split_text}, Clean:{config['cleaning_order']}")
    def plot_ax(ax, res, model_name, df_normal, df_abnormal):
        if not res: ax.set_title(f"No valid {model_name} model results"); return
        test_preds = res['test_results'].get('predictions')
        if test_preds and test_preds.get('indices') is not None:
            plot_df = pd.DataFrame({'y_true': test_preds['y_true'], 'y_pred': test_preds['y_pred'], 'mode': df_normal.loc[test_preds['indices'], 'compute_forward_mode']})
            sns.scatterplot(data=plot_df[plot_df['mode']==1], x='y_true', y='y_pred', color='blue', alpha=0.5, ax=ax, legend=False)
            sns.scatterplot(data=plot_df[plot_df['mode']==2], x='y_true', y='y_pred', color='green', alpha=0.5, ax=ax, legend=False)
        abnormal_preds = res['abnormal_results'].get('predictions')
        if abnormal_preds and abnormal_preds.get('indices') is not None:
            plot_df_ab = pd.DataFrame({'y_true': abnormal_preds['y_true'], 'y_pred': abnormal_preds['y_pred']})
            sns.scatterplot(data=plot_df_ab, x='y_true', y='y_pred', color='red', marker='x', s=100, ax=ax, legend=False)
        legend_handles = [Line2D([0], [0], marker='o', color='w', label='Prefill (Normal)', markerfacecolor='blue', markersize=10), Line2D([0], [0], marker='o', color='w', label='Decode (Normal)', markerfacecolor='green', markersize=10), Line2D([0], [0], marker='x', color='red', label='Abnormal Data', markersize=10, linestyle='None')]
        ax.legend(handles=legend_handles, title='Data Type')
        ax.set_title(get_title(res, model_name)); lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]
        ax.plot(lims, lims, 'k--', lw=2); ax.set_xlabel('Actual Duration (us)'); ax.set_ylabel('Predicted Duration (us)'); ax.set_xscale('log'); ax.set_yscale('log'); ax.set_aspect('equal', adjustable='box')
    fig, axes = plt.subplots(1, 2, figsize=(20, 9), sharey=True); fig.suptitle('Champion Model Showdown: Performance on Normal vs. Abnormal Data', fontsize=18)
    plot_ax(axes[0], best_gbdt_res, 'GBDT', df_normal, df_abnormal); plot_ax(axes[1], best_poly_res, 'Polynomial/Linear', df_normal, df_abnormal)
    axes[1].set_ylabel(''); plt.tight_layout(rect=[0, 0, 1, 0.95]); plt.show()


# ==============================================================================
# 5. Main Execution Flow
# ==============================================================================
if __name__ == '__main__':
    # This is the main entry point for the script.
    # 1. Load the 'normal' and 'abnormal' workload datasets from CSV files.
    # 2. Perform a basic pre-filtering step on the data.
    # 3. Define a grid of experiment configurations by combining different modeling
    #    approaches, feature sets, and hyperparameters.
    # 4. Execute all defined experiment configurations in parallel for efficiency.
    # 5. After all runs are complete, generate a summary table, analyze the
    #    best-performing ("champion") models, and plot their results.
    df_normal_full, df_abnormal_full = pd.DataFrame(), pd.DataFrame()
    try:
        all_required_cols = ['duration_us','compute_batch_size','compute_avg_input_length','compute_max_input_length','compute_avg_output_length','compute_max_output_length','post_batch_size','post_avg_input_length','post_max_input_length','post_avg_output_length','post_max_output_length','compute_forward_mode','post_forward_mode']
        df_normal_full = pd.read_csv('normal.csv', usecols=lambda c: c in all_required_cols)
        print(f"Successfully loaded normal dataset: normal.csv ({len(df_normal_full)} records)")
    except FileNotFoundError: print("\nError: Normal dataset 'normal.csv' not found. Exiting."); exit()
    except ValueError as e: print(f"\nError: 'normal.csv' is missing required columns. Please ensure the following columns exist: \n{all_required_cols}\nDetails: {e}"); exit()
    try:
        df_abnormal_full = pd.read_csv('abnormal.csv', usecols=lambda c: c in all_required_cols)
        print(f"Successfully loaded abnormal dataset: abnormal.csv ({len(df_abnormal_full)} records)")
    except FileNotFoundError: print("Warning: Abnormal dataset 'abnormal.csv' not found. Evaluation will proceed on normal data only.")
    except ValueError as e: print(f"\nError: 'abnormal.csv' is missing required columns. Please ensure the following columns exist: \n{all_required_cols}\nDetails: {e}"); exit()
    for name, df in [('Normal', df_normal_full), ('Abnormal', df_abnormal_full)]:
        if not df.empty:
            original_count = len(df)
            df_filtered = df[df['duration_us'] > 100].copy().reset_index(drop=True)
            if name == 'Normal': df_normal_full = df_filtered
            else: df_abnormal_full = df_filtered
            print(f"Initial filtering of {name} data (duration_us > 100): {original_count} -> {len(df_filtered)} records")
    
    model_types = ['GBDT', 'PolynomialRidge'];
    approaches = ['unified', 'split']; 
    feature_types = ['raw', 'domain']; 
    decode_kv_load_modes = ['max+max', 'avg+avg'] ; 
    log_transforms = [True, False]; 
    iqr_multipliers = [1.5, 3.0, np.inf]; 
    cleaning_orders = ['split_first']; 
    split_strategies = ['group']
    experiment_configs = []
    for model, approach, f_type, kv_mode, log, iqr, order, split_strat in product(model_types, approaches, feature_types, decode_kv_load_modes, log_transforms, iqr_multipliers, cleaning_orders, split_strategies):
        if approach == 'unified' and f_type == 'domain': continue
        if iqr == np.inf and order == 'clean_first': continue
        if f_type != 'domain' and kv_mode != decode_kv_load_modes[0]: continue
        kv_str = f"KV={kv_mode}/" if f_type == 'domain' else ""
        name = (f"M={model}/F={f_type}/A={approach}/{kv_str}L={log}/" f"IQR={'inf' if iqr == np.inf else iqr}/O={order}/S={split_strat}")
        experiment_configs.append({'name': name, 'model_type': model, 'approach': approach, 'feature_type': f_type, 'decode_kv_load_mode': kv_mode, 'log_transform': log, 'iqr_multiplier': iqr, 'cleaning_order': order, 'split_strategy': split_strat})
    
    print(f"\nStarting parallel execution of {len(experiment_configs)} experiment configurations...")
    N_JOBS = -1
    all_results = Parallel(n_jobs=N_JOBS)(
        delayed(execute_and_evaluate_strategy)(df_normal_full, df_abnormal_full, config) 
        for config in tqdm(experiment_configs, desc="Running Experiments")
    )
    
    print("\nAll experiments have been executed. Generating reports...")
    summary_df = generate_summary_table(all_results)
    
    if summary_df is not None:
        analyze_champion_models(all_results, summary_df)
        plot_champion_models(all_results, summary_df, df_normal_full, df_abnormal_full)

